<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <link rel="icon" type="image/svg+xml" href="../../favicon.svg" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta name="description" content="Why I Built a Documentation Extractor — LLMs hallucinate technical details. I needed a way to ground them in official docs, fast." />
    <title>Why I Built a Documentation Extractor — Sean Seo</title>
    <script>(function(){var t=localStorage.getItem('theme');if(t)document.documentElement.setAttribute('data-theme',t)})()</script>
  </head>
  <body>
    <!-- Blog Header -->
    <header class="blog-header">
      <div class="container blog-header__content">
        <a href="../../" class="blog-header__back">&larr; Back to Home</a>
        <div class="blog-header__meta">
          <span>Feb 6, 2026</span>
          <span>&middot;</span>
          <span>Part 1 of the Documentation Extractor Series</span>
          <span>&middot;</span>
          <span>7 min read</span>
        </div>
        <h1 class="blog-header__title">The Problem: Why I Built a Documentation Extractor</h1>
        <p class="blog-header__subtitle">
          LLMs kept hallucinating technical details. I needed a way to ground them in official documentation — but collecting the URLs was its own problem.
        </p>
      </div>
    </header>

    <!-- Article Body -->
    <article class="article">
      <h2>The Hallucination Problem</h2>
      <p>
        I was deep in a project, asking an LLM about a specific API parameter. The answer sounded authoritative — exact flag name, clear explanation, even a code example. One problem: the parameter didn't exist. The model had hallucinated it with complete confidence.
      </p>
      <p>
        This wasn't a one-off. It kept happening — with CLI flags, configuration schemas, SDK methods. The training data was either outdated or too generic. For technical work, "close enough" doesn't cut it. You need the real answer from the real documentation.
      </p>
      <p>
        That's when I discovered the power of <strong>grounding</strong>: feeding official documentation directly into an AI tool so it has the actual source of truth. Tools like NotebookLM made this workflow possible — upload sources, ask questions, get answers backed by real docs instead of training-data approximations.
      </p>
      <p>
        There was just one friction point. A big one.
      </p>

      <h2>The URL Collection Problem</h2>
      <p>
        To ground an LLM in a technology's documentation, you need the URLs. And most documentation sites are massive. Google Cloud has thousands of pages. AWS documentation spans hundreds of services. Even a focused framework like Next.js has docs spread across dozens of nested paths.
      </p>
      <p>
        Every time I wanted to create a new AI-powered reference guide, the process looked like this:
      </p>
      <ol>
        <li>Go to the documentation site</li>
        <li>Click around trying to find the sitemap</li>
        <li>Open the XML in a browser tab — wall of angle brackets</li>
        <li>Realize it's a sitemap index pointing to more sitemaps</li>
        <li>Open each nested sitemap manually</li>
        <li>Copy URLs one at a time, or try to parse the XML by eye</li>
        <li>Realize half the URLs are for older API versions you don't need</li>
        <li>Give up and just grab the top-level pages, hoping that's enough</li>
      </ol>
      <p>
        This could eat an hour or more per documentation site. And the result was always incomplete — you'd either include too much noise or miss important sections. The quality of the grounded AI depended entirely on how thorough your URL collection was, and doing it by hand guaranteed it wouldn't be thorough enough.
      </p>

      <h2>Why Existing Tools Didn't Work</h2>
      <p>
        I looked for solutions. Sitemap validators exist but they check structure, not extract URLs. Web scrapers are overkill — I didn't need page content, just the URL list. Browser-based XML viewers show the raw sitemap but don't let you filter or copy URLs in bulk.
      </p>
      <p>
        None of them solved my specific problem: <em>Given a domain, find all its sitemaps, extract every documentation URL, let me filter by keyword, and copy the result to my clipboard — in under a minute.</em>
      </p>

      <h2>So I Built One</h2>
      <p>
        The first version was a Google Apps Script web app — a single <code>Code.gs</code> backend and an <code>Index.html</code> frontend. Enter a domain, and it checks <code>robots.txt</code> for sitemap directives, falls back to <code>/sitemap.xml</code>, then recursively parses every sitemap index and URL set it finds.
      </p>
      <p>
        The URLs come back deduplicated and displayed in a clean list. A keyword filtering UI — include and exclude filters with a tag-bubble interface — lets you narrow down to exactly the paths you care about. One click copies the filtered list to your clipboard. Paste it into NotebookLM and you've got an expert guide grounded in official sources.
      </p>
      <p>
        The first time I used it on <code>cloud.google.com</code>, it discovered 8 nested sitemaps and extracted over 4,000 documentation URLs in about 15 seconds. I filtered down to the Cloud Run docs, copied 200 URLs, and pasted them into NotebookLM. Suddenly I had a chatbot that could answer Cloud Run questions with citations to the actual documentation — no hallucinations.
      </p>
      <p>
        That moment — going from "I need to read 200 doc pages" to "I have an expert I can ask" — was when I knew this workflow was something bigger than a utility script.
      </p>

      <h2>What I Didn't Expect</h2>
      <p>
        The Documentation Extractor was supposed to be a personal tool. A quick script to save me time before building NotebookLM notebooks. But the workflow it enabled — <strong>automated doc discovery → filtered extraction → AI grounding</strong> — turned out to be the seed of something larger.
      </p>
      <p>
        What if instead of just extracting URLs for manual pasting, the tool could also fetch the content, process it, and power a chatbot directly? What if the sitemap extraction was just the first step in a pipeline that turned any documentation site into a conversational AI?
      </p>
      <p>
        That's how <strong>DocWeb</strong> was born — a full sitemap extraction and chatbot platform that evolved directly from this script. The Documentation Extractor is the "v0" prototype, the proof of concept that validated the core workflow. DocWeb takes it the rest of the way.
      </p>
      <p>
        But I'm keeping the original tool alive. It's open source, it's free, and it solves a real problem in under a minute. Sometimes the simplest version is the one people actually use.
      </p>

      <hr />

      <p>
        <strong>Next up:</strong> In Part 2, I'll dive into the technical side — how sitemap parsing works, the edge cases in XML sitemap structures, and how batch fetching with exponential backoff keeps the tool reliable even on massive documentation sites.
      </p>
    </article>

    <!-- Blog CTA -->
    <div class="blog-cta">
      <div class="blog-cta__card">
        <h2 class="blog-cta__title">See It in Action</h2>
        <p class="blog-cta__text">Check out the Documentation Extractor project page for tech stack details, architecture overview, and the live demo.</p>
        <div class="blog-cta__links">
          <a href="../../projects/url-extractor/" class="btn btn--primary">View the Project</a>
          <a href="../../" class="btn btn--outline">Back to Home</a>
        </div>
      </div>
    </div>

    <!-- Series Nav -->
    <div class="series-nav">
      <div class="series-nav__inner">
        <div></div>
        <a href="../../blog/parsing-sitemaps-at-scale/" class="series-nav__link">
          <span class="series-nav__label">Next in Series</span>
          Part 2: Parsing Sitemaps at Scale
        </a>
      </div>
    </div>

    <script type="module" src="/src/blog/why-i-built-url-extractor.js"></script>
  </body>
</html>
